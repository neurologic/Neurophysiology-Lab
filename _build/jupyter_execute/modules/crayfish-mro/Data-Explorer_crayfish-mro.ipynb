{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32647951-7dcf-4553-b42a-5d8508d95cf9",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/neurologic/Neurophysiology-Lab/blob/main/week-5/Sensory-Coding-MRO.ipynb\" target=\"_blank\" rel=\"noopener noreferrer\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b445ff-7cf7-442e-bb25-83afe6d5a3fb",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "# Sensory Coding - MROs\n",
    "\n",
    "There are two MROs innervating the deep flexor muscles, which can be distinguished based on spike height and shape. \n",
    "\n",
    "What are the dynamic properties of MRO sensory responses? In this notebook, you will process, analyze, and model MRO sensory responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07451ff-cf56-4763-b4ac-bd099ba1f145",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Setup](#setup)\n",
    "- [Part I. Process Data](#one)\n",
    "- [Part II. Analyze Processed Data](#two)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d476b-9387-43d7-b4bf-0528d9f4950a",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# Setup\n",
    "\n",
    "[toc](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f344a-8c2a-4080-b972-8e7a30c91e66",
   "metadata": {},
   "source": [
    "Import and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70dacf-b862-4afa-8fc0-dce8c363fdf9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode: \"form\"}\n",
    "\n",
    "#@markdown Run this code cell to import packages and define functions \n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import ndimage, optimize\n",
    "from scipy.signal import hilbert,medfilt,resample, find_peaks\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime,timezone,timedelta\n",
    "pal = sns.color_palette(n_colors=15)\n",
    "pal = pal.as_hex()\n",
    "\n",
    "from ipywidgets import interactive, HBox, VBox, widgets, interact\n",
    "\n",
    "def monoExp(x, m, t, b):\n",
    "    return m * np.exp(-x / t) + b\n",
    "\n",
    "print('Task completed at ' + str(datetime.now(timezone(-timedelta(hours=5)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a09e1-c305-4809-8302-c1fad4140efa",
   "metadata": {},
   "source": [
    "Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a4500-470b-44e3-8572-a0d55f3bc1cb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode: \"form\"}\n",
    "\n",
    "#@markdown Run this cell to mount your Google Drive.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print('Task completed at ' + str(datetime.now(timezone(-timedelta(hours=5)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa1776-ad44-4487-ad14-9dfe050de305",
   "metadata": {},
   "source": [
    "Import data digitized with *Nidaq USB6211* and recorded using *Bonsai-rx* as a *.bin* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7152a58-2458-4b02-a680-e40e8b706c95",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode: \"form\"}\n",
    "\n",
    "#@markdown Specify the file path \n",
    "#@markdown to your recorded data on Drive (find the filepath in the colab file manager:\n",
    "\n",
    "filepath = \"full filepath goes here\"  #@param \n",
    "# filepath = \"/Users/kperks/mnt/OneDrive - wesleyan.edu/Teaching/Neurophysiology_FA22/data/20220609/MRO2022-06-09T12_59_19.bin\"\n",
    "\n",
    "#@markdown Specify the sampling rate and number of channels recorded.\n",
    "sampling_rate = None #@param\n",
    "number_channels = None #@param\n",
    "channel_to_process = 0 #@param\n",
    "\n",
    "# sampling_rate = 30000 #@param\n",
    "# number_channels = 1 #@param\n",
    "\n",
    "downsample = False #@param\n",
    "newfs = 10000 #@param\n",
    "\n",
    "#@markdown After you have filled out all form fields, \n",
    "#@markdown run this code cell to load the data. \n",
    "\n",
    "filepath = Path(filepath)\n",
    "\n",
    "# No need to edit below this line\n",
    "#################################\n",
    "data = np.fromfile(Path(filepath), dtype = np.float64)\n",
    "data = data.reshape(-1,number_channels)\n",
    "data = data-data[0,:] # only do this offset adjustment for motor nerve recordings\n",
    "\n",
    "dur = np.shape(data)[0]/sampling_rate\n",
    "print('duration of recording was %0.2f seconds' %dur)\n",
    "\n",
    "fs = sampling_rate\n",
    "if downsample:\n",
    "    # newfs = 2500 #downsample data\n",
    "    chunksize = int(sampling_rate/newfs)\n",
    "    if number_channels>1:\n",
    "        data = data[0::chunksize,:]\n",
    "    if number_channels==1:\n",
    "        data = data[0::chunksize]\n",
    "    fs = int(np.shape(data)[0]/dur)\n",
    "\n",
    "time = np.linspace(0,dur,np.shape(data)[0])\n",
    "\n",
    "if len(np.shape(data))>1:\n",
    "    channel = channel_to_process\n",
    "    channel_signal = data[:,channel]\n",
    "if len(np.shape(data))==1:\n",
    "    channel_signal = data\n",
    "\n",
    "print('Now be a bit patient while it plots.')\n",
    "\n",
    "f = go.FigureWidget(make_subplots(rows=1, cols=1, shared_xaxes= True)) #,layout=go.Layout(height=500, width=800))\n",
    "f.add_trace(go.Scatter(x = time[0:fs], y = channel_signal[0:fs],\n",
    "                             opacity=1),row=1,col=1)\n",
    "\n",
    "f.update_layout(height=600, width=800,\n",
    "                showlegend=False,\n",
    "               xaxis_title=\"time(seconds)\", \n",
    "                  yaxis_title='voltage')\n",
    "\n",
    "slider = widgets.FloatRangeSlider(\n",
    "    min=0,\n",
    "    max=dur,\n",
    "    value=(0,1),\n",
    "    step= 1,\n",
    "    readout=False,\n",
    "    description='Time')\n",
    "slider.layout.width = '600px'\n",
    "\n",
    "# our function that will modify the xaxis range\n",
    "def response(x):\n",
    "    with f.batch_update():\n",
    "        starti = int(x[0]*fs)\n",
    "        stopi = int(x[1]*fs)\n",
    "        f.data[0].x = time[starti:stopi]\n",
    "        f.data[0].y = channel_signal[starti:stopi]\n",
    "\n",
    "vb = VBox((f, interactive(response, x=slider)))\n",
    "vb.layout.align_items = 'center'\n",
    "vb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b40cda-55dd-4604-afb7-89e5d0731983",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "\n",
    "# Part I. Process Data\n",
    "\n",
    "[toc](#toc)\n",
    "\n",
    "Decide what portion of the raw data you want to analyze (maybe your data file contains multiple conditions, maybe there was some bad noise during part of your recording, or the nerve was lost before the end of the recording, or you accidentally forgot to stop the recording, etc...). \n",
    "\n",
    "If you want to analyze spiking activity (calculating rate, for example), you could look through your raw data and write down each time you see a spike. \n",
    "\n",
    "There are lots of computational tools for automating this process. Computers can also help you extract patterns from large amounts of data (for example different \"classes\" of spike waveform). In neuroscience research, the process of clustering spiking events based on their amplitude and waveform is termed the \"spike sorting.\"\n",
    "\n",
    "For this dataset, you will proocess the data using the following sequence of steps:\n",
    "\n",
    "1. [Select Data](#select-data)\n",
    "2. [Detect Peaks](#detect-spikes)\n",
    "3. [Cluster Events](#cluster-events)\n",
    "\n",
    "    a. [Kmeans](#kmeans)\n",
    "    \n",
    "    b. [Visualize](#display-clusters)\n",
    "    \n",
    "    c. [Merge Clusters](#merge-clusters)\n",
    "    \n",
    "    d. [Check putative unit identity in raw data](#raw-cluster-scatter)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790c075d-0374-4b02-9184-790b4bbbbf61",
   "metadata": {},
   "source": [
    "<a id='select-data'></a>\n",
    "\n",
    "## 1. Select Data\n",
    "\n",
    "Specify the timerange for the data you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd85ab4-2d89-4332-b7fd-3ad15a7ca289",
   "metadata": {
    "cellView": "form",
    "id": "iE2MylaxTGpL",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Type in the start and stop time (in seconds) \n",
    "#@markdown that specifies the section of your recording you want to focus on for analysis.\n",
    "start_time =   None #@param {type: \"number\"}\n",
    "stop_time = None  #@param {type: \"number\"}\n",
    "\n",
    "# start_time =   0 #@param {type: \"number\"}\n",
    "# stop_time = 153  #@param {type: \"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b822981-388d-4b91-979e-3e7ddf3cc94c",
   "metadata": {},
   "source": [
    "<a id='detect-spikes'></a>\n",
    "\n",
    "## 2. Detect Peaks\n",
    "Detect peaks in the signal\n",
    "\n",
    "First, in the code cell below, write a simple script that:\n",
    "- calculates the standard deviation of voltage in the raw signal using the ```np.std()``` module to store the result as a variable called \"<b>SD</b>\". \n",
    "- uses the ```print()``` function to print the value stored in the variable ```SD``` as an output of the code cell. \n",
    "- calculates the value equal to 5 times the standard deviation and stores the result as a variable called \"<b>threshold</b>\". \n",
    "- uses the ```print()``` function to print the value stored in the variable ```threshold``` as an output of the code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce173e-a7fd-4577-837a-d76845df6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93145b35-d81f-4085-a24f-691e0f8f5dfe",
   "metadata": {},
   "source": [
    "Then use the result to determine a spike detection threshold used by the ```find peaks``` algorithm in the following code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a47f3-7a95-4289-adc6-9a3bbc60866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape([0]*sum(inwin_inds))\n",
    "inwin_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a96f86-cc03-489d-9971-1943f39b8d3d",
   "metadata": {
    "cellView": "form",
    "id": "iE2MylaxTGpL",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Type in the threshold amplitude for event detection determined by your SD calculations.\n",
    "spike_detection_threshold = None  #@param {type: \"number\"}\n",
    "# spike_detection_threshold = 0.1  #@param {type: \"number\"}\n",
    "\n",
    "#@markdown Then from the dropdown, select a polarity (whether peaks are up or down)\n",
    "peaks = \"select peak direction\"  #@param ['select peak direction','up', 'down']\n",
    "# peaks = \"up\"  #@param ['select peak direction','up', 'down']\n",
    "\n",
    "#@markdown Finally, run this cell to set these values and plot a histogram of peak amplitudes.\n",
    "\n",
    "\n",
    "if peaks=='up': polarity = 1\n",
    "if peaks=='down': polarity=-1\n",
    "\n",
    "min_isi = 0.001 #seconds\n",
    "\n",
    "peaks,props = find_peaks(polarity * channel_signal,height=spike_detection_threshold, \n",
    "                         prominence = spike_detection_threshold, distance=int(min_isi*fs))\n",
    "peaks_t = peaks/fs\n",
    "inwin_inds = ((peaks_t>start_time) & (peaks_t<stop_time))\n",
    "df_props = pd.DataFrame({\n",
    "        'height': props['peak_heights'][inwin_inds],\n",
    "        'spikeT' : peaks_t[inwin_inds],\n",
    "        'spikeInd' : peaks[inwin_inds],\n",
    "        'cluster' : [0]*sum(inwin_inds)\n",
    "            })\n",
    "\n",
    "bins = np.linspace(0,np.abs(np.max(polarity*channel_signal)),200)\n",
    "n,_ = np.histogram(df_props['height'],bins = bins) # calculate the histogram\n",
    "hfig,ax = plt.subplots(1)\n",
    "ax.step(bins[1:],n,color='black')\n",
    "ax.set_ylabel('count',fontsize=14)\n",
    "ax.set_xlabel('amplitude',fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "windur = 0.003\n",
    "winsamp = int(windur*fs)\n",
    "spkarray = []\n",
    "for i in df_props['spikeInd'].values:\n",
    "    spkarray.append(channel_signal[i-winsamp : i+winsamp+1])\n",
    "\n",
    "df = pd.DataFrame(np.asarray(spkarray).T)\n",
    "df_norm =(df - df.mean()) / df.std() # normalize for pca\n",
    "\n",
    "n_components=5 #df.shape[0] \n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(df_norm)\n",
    "df_pca = pd.DataFrame(pca.transform(df), columns=['PC%i' % i for i in range(n_components)], index=df.index)\n",
    "print('You detected %i events above threshold.' %len(df.columns))\n",
    "\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=df_pca.columns, index=df.columns)\n",
    "df_data = loadings.join(df_props['height'])\n",
    "\n",
    "hfig,ax = plt.subplots(1)\n",
    "ax.set_xlabel('event time (sec)',fontsize=14)\n",
    "ax.set_ylabel('amplitude (volts)',fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.set_ylim(0,np.abs(np.max(polarity*channel_signal)))\n",
    "ax.scatter(df_props['spikeT'],df_props['height'],color='black')\n",
    "\n",
    "print('Tasks completed at ' + str(datetime.now(timezone(-timedelta(hours=5)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18e966-a9c1-4bfb-8740-6d2c7dbeb376",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id = \"cluster-events\"></a>\n",
    "\n",
    "## 3. Cluster\n",
    "Cluster events if needed. \n",
    "\n",
    "The histogram plot produced in the last step can give you a sense for how many distinct neurons might be in your recording. The scatter plot of peak amplitude across time can give you a sense for how stable the recording was. If you had good recording stability, you can cluster spike events categorically to analyze the activity of individual neurons independently. \n",
    "\n",
    "If your recording is not stable, or is too noisy, then you may not be able to distinguish cell types. In this case, skip this clustering step. You will only have one cluster and its identity will be '0'. \n",
    "\n",
    "Clustering steps:\n",
    "- [Kmeans](#kmeans)\n",
    "- [Visualize](#display-clusters)\n",
    "- [Merge Clusters](#merge-clusters)\n",
    "- [Check event categorization against raw data](#raw-cluster-scatter)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa89b0f-19a4-4fbe-8ccb-c569cab88d50",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='kmeans'></a>\n",
    "\n",
    "### Kmeans\n",
    "\n",
    "We can cluster events based on peak height and waveform shape using [\"Kmeans\"](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering. \n",
    "This will provide us with \"putative single units\" for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1f5d0-54ab-4cab-9076-5b2d872580b4",
   "metadata": {
    "cellView": "form",
    "id": "4e574ce9-d314-4a20-918b-f2496260c9a8",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Choose the number of clusters you want to split the event-based data into and type that number below. <br>\n",
    "#@markdown >Note: It can sometimes help to \"over-split\" the events into more clusters \n",
    "#@markdown than you think will be necessary. You can try both strategies and assess the results.\n",
    "number_of_clusters = None #@param\n",
    "# number_of_clusters = 1\n",
    "#@markdown Then run this cell to run the Kmeans algorithm. \n",
    "\n",
    "# No need to edit below this line\n",
    "#################################\n",
    "\n",
    "kmeans = KMeans(n_clusters=number_of_clusters).fit(df_data)\n",
    "# df_props['peaks_t'] = peaks_t\n",
    "df_props['cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29851d8d-403c-46b4-884d-45b923686fa1",
   "metadata": {},
   "source": [
    "<a id = \"display-clusters\"></a>\n",
    "\n",
    "### Visualize \n",
    "\n",
    "Now that the events are clustered, you can visualize the mean spike waveform associated with each cluster (putative motor neuron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f188f71-cf6c-4fc4-ad24-b43b9ec5544c",
   "metadata": {
    "cellView": "form",
    "id": "4e574ce9-d314-4a20-918b-f2496260c9a8",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:'form'}\n",
    "\n",
    "#@markdown Run this cell to display the mean (and std) waveform for each cluster.\n",
    "\n",
    "#@markdown Specify the time before and after each event peak to plot.\n",
    "windur = 0.003 #@param\n",
    "\n",
    "winsamps = int(windur * fs)\n",
    "x = np.linspace(-windur,windur,winsamps*2)*1000\n",
    "hfig,ax = plt.subplots(1,figsize=(8,6))\n",
    "ax.set_ylabel('Volts recorded',fontsize=14)\n",
    "ax.set_xlabel('milliseconds',fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "for k in np.unique(df_props['cluster']):\n",
    "    spkt = df_props.loc[df_props['cluster']==k]['spikeT'].values #['peaks_t'].values\n",
    "    spkt = spkt[(spkt>windur) & (spkt<(len(channel_signal)/fs)-windur)]\n",
    "    print(str(len(spkt)) + \" spikes in cluster number \" + str(k))\n",
    "    spkwav = np.asarray([channel_signal[(int(t*fs)-winsamps):(int(t*fs)+winsamps)] for t in spkt])\n",
    "    wav_u = np.mean(spkwav,0)\n",
    "    wav_std = np.std(spkwav,0)\n",
    "    ax.plot(x,wav_u,linewidth = 3,label='cluster '+ str(k),color=pal[k])\n",
    "    ax.fill_between(x, wav_u-wav_std, wav_u+wav_std, alpha = 0.25,color=pal[k])\n",
    "plt.legend(bbox_to_anchor=[1.25,1],fontsize=14);\n",
    "\n",
    "print('Tasks completed at ' + str(datetime.now(timezone(-timedelta(hours=5)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a37aa64-ee1b-4248-8901-982f326e0cb0",
   "metadata": {
    "id": "9iyWsThmXdI_"
   },
   "source": [
    "<a id='merge-clusters'></a>\n",
    "\n",
    "### Merge Clusters\n",
    "You can skip this step if unecessary. \n",
    "If there are multiple spike clusters you want to merge into a single cell class, *edit and run* the cell below.\n",
    "\n",
    "> **merge_cluster_list** = a list of the clusters (identified by numbers associated with the colors specified in the legend above).\n",
    "  - **For example**, the folowing list would merge clusters 0 and 2 together and 1, 4, and 3 together: <br>\n",
    "     **merge_cluster_list = [[0,2],[1,4,3]]**\n",
    "  - For each merge group, the first cluster number listed will be the re-asigned cluster number for that group (for example, in this case you would end up with a cluster number 0 and a cluster number 1). \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae4811-90b0-4c5d-8f7b-39d48fb2590b",
   "metadata": {
    "cellView": "form",
    "id": "EDJgd8DAXRba",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "\n",
    "#@markdown ONLY USE THIS CODE CELL IF YOU WANT TO MERGE CLUSTERS. \n",
    "#@markdown OTHERWISE, MOVE ON. \n",
    "#@markdown <br> Below, create your list (of sublists) of clusters to merge.\n",
    "#@markdown >Just leave out from the list any clusters that you want unmerged.\n",
    "merge_cluster_list = [[0,3,4],[1,2]] #@param\n",
    "#@markdown Then, run this cell to merge clusters as specified.\n",
    "\n",
    "for k_group in merge_cluster_list:\n",
    "    for k in k_group:\n",
    "        df_props.loc[df_props['cluster']==k,'cluster'] = k_group[0]\n",
    "print('you now have the following clusters: ' + str(np.unique(df_props['cluster'])))\n",
    "\n",
    "print('Tasks completed at ' + str(datetime.now(timezone(-timedelta(hours=5)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba23b1-7643-4138-9b70-8478910de0c4",
   "metadata": {},
   "source": [
    "After merging, return to the [display clusters](#display-clusters) code cell to plot the mean waveform of each new cluster (and determine if you need to [merge more](#merge-clusters))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a51f2-a02c-4e03-a227-6079e348bce6",
   "metadata": {},
   "source": [
    "<a id=\"raw-cluster-scatter\"></a>\n",
    "\n",
    "### Check event categorization against raw data.\n",
    "\n",
    "Once you are happy with the clustering results based on the waveform shapes, check back with the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ccb8b-9d2b-4c66-8939-59c0b6f51df1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Run this cell to overlay spike times by cluster identity on the raw signal\n",
    "\n",
    "f = go.FigureWidget()\n",
    "f.add_trace(go.Scatter(x = time[0:fs], y = channel_signal[0:fs],\n",
    "                             name='pre synaptic',opacity=1,line_color='black'))\n",
    "for k in np.unique(df_props['cluster']):\n",
    "    start = 0 \n",
    "    stop = 1\n",
    "    inwin_inds = np.asarray([(df_props['spikeT'].values>start) & (df_props['spikeT'].values<stop)]).T\n",
    "    df_ = df_props[inwin_inds]\n",
    "    df_ = df_[df_['cluster']==k]\n",
    "    \n",
    "    f.add_trace(go.Scatter(x = df_['spikeT'], y = polarity*df_['height'],\n",
    "                             line_color=pal[k],name=str(k) + ' times',mode='markers'))\n",
    "    \n",
    "f.update_layout(height=600, width=800,\n",
    "               xaxis_title=\"time(seconds)\", \n",
    "                  yaxis_title='amplitude (volts)')\n",
    "\n",
    "\n",
    "slider = widgets.FloatRangeSlider(\n",
    "    min=0,\n",
    "    max=dur,\n",
    "    value=(0,1),\n",
    "    step= 1,\n",
    "    readout=False,\n",
    "    description='Time')\n",
    "slider.layout.width = '800px'\n",
    "\n",
    "# our function that will modify the xaxis range\n",
    "def response(x):\n",
    "    with f.batch_update():\n",
    "        starti = int(x[0]*fs)\n",
    "        stopi = int(x[1]*fs)\n",
    "        f.data[0].x = time[starti:stopi]\n",
    "        f.data[0].y = channel_signal[starti:stopi]\n",
    "        for i,k in enumerate(np.unique(df_props['cluster'])):\n",
    "            inwin_inds = np.asarray([(df_props['spikeT'].values>x[0]) & (df_props['spikeT'].values<x[1])]).T\n",
    "            df_ = df_props[inwin_inds]\n",
    "            df_ = df_[df_['cluster']==k]\n",
    "            f.data[1+i].x = df_['spikeT']\n",
    "            f.data[1+i].y = polarity*df_['height']\n",
    "\n",
    "vb = VBox((f, interactive(response, x=slider)))\n",
    "vb.layout.align_items = 'center'\n",
    "vb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f73ef1-6320-408a-87c7-15b3ce04256d",
   "metadata": {},
   "source": [
    "If you think that two different spike waveforms are being lumped together, try going back to the [Kmeans clustering algorithm](#kmeans) and increasing the cluster number constraint on the Kmeans algorithm - then [merge](#merge-clusters) as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd727a14-c83f-4bc1-91d7-59cb24049341",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "# Part II. Analyze Processed Data \n",
    "\n",
    "[toc](#toc)\n",
    "\n",
    "This section provides tools to obtain two metrics of MRO spiking responses: ***spike rate*** and ***spike rate adaptation***.\n",
    "\n",
    "First, you will plot the inter-spike interval (ISI; the time between spikes) for the event cluster that you want to analyze. You will also obtain a plot of the stimulus signal aligned in time to the spiking data.\n",
    "\n",
    "From this plot, you should be able to determine 'trial' times for a trial-based analysis of spike rate and adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c969d-59ee-4717-83f4-45ceae29dae0",
   "metadata": {},
   "source": [
    "## Plot continuous ISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3646f36-0243-42fe-85dd-270415da26ef",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Run this code cell to plot:  \n",
    "#@markdown - a scatter of ISI across time for individual clusters  \n",
    "#@markdown - the stimulus signal\n",
    "#@markdown > Once you run this code cell a first time, you will be able to select different clusters from the dropdown menu to change the plot data accordingly.\n",
    "\n",
    "\n",
    "k = df_props['cluster'][0] #seed it to start\n",
    "spkt = df_props.loc[df_props['cluster']==k]['spikeT'].values\n",
    "isi = np.diff(spkt)\n",
    "\n",
    "f = go.FigureWidget(make_subplots(rows=1,cols=1))\n",
    "f.add_trace(go.Scatter(x = spkt[1:], y = isi, line_color = 'black', mode='markers'),row=1,col=1)\n",
    "\n",
    "f.update_layout(height=600, width=800,\n",
    "                showlegend=False,\n",
    "                xaxis_title=\"isi (seconds)\",\n",
    "                  yaxis_title='isi (seconds)')\n",
    "\n",
    "cluster_select = widgets.Dropdown(\n",
    "    options=np.unique(df_props['cluster']),\n",
    "    value=k,\n",
    "    description='Cluster ID:',\n",
    "    disabled=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# our function that will modify the xaxis range\n",
    "def response(k):\n",
    "    with f.batch_update():\n",
    "        spkt = df_props.loc[df_props['cluster']==k]['spikeT'].values\n",
    "        isi = np.diff(spkt)\n",
    "        \n",
    "        f.data[0].x = spkt[1:]\n",
    "        f.data[0].y = isi\n",
    "\n",
    "\n",
    "vb = VBox((f, interactive(response, k=cluster_select)))\n",
    "vb.layout.align_items = 'center'\n",
    "vb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b39ce-01e0-42c3-bd00-1901037ebe9b",
   "metadata": {},
   "source": [
    "## Plot trial rate overlaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fbec2-3484-483d-91b1-7df4af5c738d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Determine the start time for each *trial*.  \n",
    "#@markdown What defines the start of a trial will depend on your question.  \n",
    "#@markdown Enter the trial times as a list below.\n",
    "\n",
    "trials_list = [51.72, 61.65, 71.256, 83.063] #@param\n",
    "\n",
    "# # different amplitudes\n",
    "# trials = [6.345,13.836,21.63,32.635,40.319]\n",
    "# trials = [51.72, 61.65, 71.256, 80.063]\n",
    "# trials = [94.692,103.73,111.665,120.871,129.286]\n",
    "\n",
    "# # offset response\n",
    "# trials = [99.832,110,118.327,126.86]\n",
    "\n",
    "#@markdown How much time before and after the trial time do you want to visualize?\n",
    "window = 10\n",
    "\n",
    "#@markdown Run this code cell to plot the result (for the cluster you selected in the first code cell of this section)\n",
    "\n",
    "f = go.FigureWidget(make_subplots(rows=1,cols=1))\n",
    "\n",
    "for t in trials_list:\n",
    "    ti = np.argmin(np.abs(spkt-t))\n",
    "    sweep = spkt[(spkt>spkt[ti]-10) & (spkt<spkt[ti]+10)]-spkt[ti]\n",
    "    sweep_rate = 1/np.diff(sweep)\n",
    "    f.add_trace(go.Scatter(x = sweep[1:], y = sweep_rate),row=1,col=1)\n",
    "\n",
    "\n",
    "f.update_layout(height=500, width=800,\n",
    "                showlegend=False,\n",
    "                xaxis_title=\"time (seconds)\",\n",
    "                  yaxis_title='rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbbec7-827d-44e6-b272-fec78b634bb3",
   "metadata": {},
   "source": [
    "## Fitting an exponential model\n",
    "\n",
    "You can create a ***model*** of the MRO response. If the model is correct, you should be able to 'explain' the response in terms of the model (it should be able to predict all features of the response).\n",
    "\n",
    "The following code cells provide a tool to model the response using a single exponential function of the form:\n",
    "\n",
    "$$\n",
    "\\Large y = a + b*e^{-x/\\tau}\n",
    "$$\n",
    "\n",
    "For spiking adaptation, the following equation variables are more intuitive:\n",
    "\n",
    "$$\n",
    "\\Large R_{t} = R_{\\infty} + R_{0}e^{−t/\\tau}\n",
    "$$\n",
    "\n",
    "Where $R_{t}$ is the firing rate at time $t$, $R_{\\infty}$ is the calculated firing rate if this degree of stretch were maintained infinitely, $R_{\\infty}$ + $R_{0}$ is the calculated “initial” peak firing rate at time 0, and $\\tau$ is the adaptation rate. The adaptation rate is the time it takes the spike rate to fall by a factor of $1/e$.\n",
    "\n",
    "To model the MRO response in this form, you will *fit* your data to the model. \n",
    "\n",
    "In biology / electrophysiology *bi*exponential functions are often used to separate fast and slow components of exponential decay which may be caused by different mechanisms and occur at different rates. In this example we will only fit the data to a method with a exponential component (a *mono*exponential function), but the idea is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbee044-ee56-4d8d-8c5f-93ba0828446d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Fitting the model on a single trial\n",
    "\n",
    "#@markdown Specify a single trial time (t), \n",
    "#@markdown the duration of the trial (trial_dur; after trial onset), \n",
    "#@markdown and initial estimates for the model parameters (parameter_initialize).\n",
    "t = 83.063 #@param\n",
    "trial_dur = 10 #@param\n",
    "parameter_initialize = (1, 1, 10) #@param # start with values near those we expect\n",
    "\n",
    "ti = np.argmin(np.abs(spkt-t))\n",
    "xs = spkt[(spkt>=spkt[ti]) & (spkt<spkt[ti]+trial_dur)]-spkt[ti]\n",
    "ys = 1/np.diff(xs)\n",
    "xs = xs[1:]\n",
    "\n",
    "# perform the fit\n",
    "params_, cv = optimize.curve_fit(monoExp, xs, ys, parameter_initialize)\n",
    "m, t, b = params_\n",
    "\n",
    "# determine quality of the fit\n",
    "squaredDiffs = np.square(ys - monoExp(xs, m, t, b))\n",
    "squaredDiffsFromMean = np.square(ys - np.mean(ys))\n",
    "rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "\n",
    "# inspect the parameters\n",
    "print('What are the model parameters?')\n",
    "print(f\"Y = {m:.2f} * e^(-x / {t:.2f}) + {b:.2f}\")\n",
    "\n",
    "print('')\n",
    "\n",
    "# report the fit\n",
    "print('How good is the model fit to your data?')\n",
    "print(f\"R² = {rSquared:.2f}\")\n",
    "\n",
    "f = go.FigureWidget(make_subplots(rows=1,cols=1))\n",
    "\n",
    "f.add_trace(go.Scatter(x = xs, y = ys, line_color = 'black', name = 'data', mode='markers'),row=1,col=1)\n",
    "f.add_trace(go.Scatter(x = xs, y = monoExp(xs, m, t, b), line_color = 'green', name= 'fit'),row=1,col=1)\n",
    "\n",
    "f.update_layout(height=500, width=800,\n",
    "                showlegend=False,\n",
    "                xaxis_title=\"time (seconds)\",\n",
    "                  yaxis_title='rate')\n",
    "\n",
    "# # plot the results\n",
    "# plt.plot(xs, ys, '.', label=\"data\")\n",
    "# plt.plot(xs, monoExp(xs, m, t, b), '--', label=\"fitted\")\n",
    "# plt.title(\"Fitted Exponential Curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb07a15-a6ca-4cbb-a852-c05465fb747b",
   "metadata": {},
   "source": [
    "There is likely *across trial* variability in the MRO response. You can get a model fit for each trial and then evaluate the distrubution of model parameters. Alternatively, you could take the average response across trials and fit the model to the average. Here, you will do the former. What are the pros and cons of each of these methods in terms of statistical results? In other words, are the two methods equivalent or can you calculate different metrics of statistics depending on the method? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766c2eb-3e33-4078-b4d3-a00f61592288",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Fitting the model across multiple trials\n",
    "\n",
    "#@markdown Specify a list of trials times, the duration of the trial (after trial onset), and estimates for the model parameters.\n",
    "trial_list = [94.692,103.73,111.665,120.871,129.286] #@param \n",
    "trial_dur = 3 #@param\n",
    "parameter_initialize = (1, 1, 10) #@param # start with values near those we expect\n",
    "\n",
    "# plt.figure()\n",
    "params = []\n",
    "for t in trial_list:\n",
    "    ti = np.argmin(np.abs(spkt-t))\n",
    "    xs = spkt[(spkt>=spkt[ti]) & (spkt<spkt[ti]+trial_dur)]-spkt[ti]\n",
    "    ys = 1/np.diff(xs)\n",
    "    xs = xs[1:]\n",
    "    \n",
    "    params_, cv = optimize.curve_fit(monoExp, xs, ys, parameter_initialize)\n",
    "    m, t, b = params_\n",
    "    \n",
    "    squaredDiffs = np.square(ys - monoExp(xs, m, t, b))\n",
    "    squaredDiffsFromMean = np.square(ys - np.mean(ys))\n",
    "    rSquared = 1 - np.sum(squaredDiffs) / np.sum(squaredDiffsFromMean)\n",
    "    params_ = np.concatenate([params_, [rSquared]])\n",
    "    \n",
    "    params.extend(params_)\n",
    "\n",
    "fit_parameters = np.asarray(params).reshape(-1,4)\n",
    "\n",
    "u_fit = np.mean(fit_parameters,0)\n",
    "std_fit = np.std(fit_parameters,0)\n",
    "\n",
    "\n",
    "# inspect the results\n",
    "print('results based on mean across trials:')\n",
    "print(f\"R² = {u_fit[3]:0.2f}\")\n",
    "print(f\"Y = {u_fit[0]:0.2f} * e^(-x / {u_fit[1]:0.2f}) + {u_fit[2]:0.2f}\")\n",
    "print('')\n",
    "\n",
    "print('array of fit parameters across all trials: (R0, tau, Rinf, R2)')\n",
    "print(fit_parameters)\n",
    "print('')\n",
    "\n",
    "print('mean fit parameters: (R0, tau, Rinf, R2)')\n",
    "print(u_fit)\n",
    "print('')\n",
    "print('standard deviation fit parameters: (R0, tau, Rinf, R2)')\n",
    "print(std_fit)\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195633c-88ba-46bd-a379-2e5597a9681f",
   "metadata": {},
   "source": [
    "## Generate model data\n",
    "\n",
    "Now, use the mean parameters to plot the model equations.\n",
    "For multiple conditions, list multiple values for each parameter. Keep the order of conditions the same in each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8754bdd1-2e07-4a0b-b83c-24f859a97173",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Create a list of conditions\n",
    "condition = [1,2,3]\n",
    "\n",
    "#@markdown Create a list of parameters across conditions\n",
    "m = [7.7, 7.36, 7.05] #@param\n",
    "tau = [0.6, 0.49, 0.48] #@param\n",
    "b = [10.54, 13.63, 14.88] #@param\n",
    "r_squared = [0.94, 0.96, 0.94]#@param\n",
    "\n",
    "#@markdown How long do you want the model MRO to be stretched for under each condition?\n",
    "trial_dur = 10 #@param\n",
    "\n",
    "# Run this code cell to plot the model across conditions\n",
    "f = go.FigureWidget(make_subplots(rows=1,cols=1))\n",
    "\n",
    "x_ = np.linspace(0,trial_dur,trial_dur*100)\n",
    "\n",
    "for m_,tau_,b_,c_ in zip(m,tau,b,condition):\n",
    "    y_ = monoExp(x_, m_,tau_,b_)\n",
    "    f.add_trace(go.Scatter(x = x_, y = y_, name = str(c_), mode='lines'),row=1,col=1)\n",
    "\n",
    "f.update_layout(height=500, width=600,\n",
    "                showlegend=True,\n",
    "                xaxis_title=\"time (seconds)\",\n",
    "                  yaxis_title='rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b8788-0286-48f6-b57d-d8bdfa9d1cb7",
   "metadata": {},
   "source": [
    "## Inspect model parameters across conditions\n",
    "\n",
    "Plot the model parameters as a function of stretch. Is there some predictable relationship between these?\n",
    "Which parameters are a function of specific stretch amplitudes? Which parameters are independent of the specific stretch amplitude? In other words, which properties of the MRO response code for position of the tail and which code for other aspects of the movement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708364c5-19a5-4e67-ac61-903f932bd642",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title {display-mode:\"form\"}\n",
    "\n",
    "#@markdown Run this code cell to plot parameters across conditions. \n",
    "\n",
    "hfig,ax = plt.subplots(1,4)\n",
    "ax[0].plot(condition,m)\n",
    "ax[1].plot(condition,t)\n",
    "ax[2].plot(condition,b)\n",
    "ax[3].plot(condition,r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451b557-6777-4492-955e-a41130ad39b9",
   "metadata": {},
   "source": [
    "<hr> \n",
    "Written by Dr. Krista Perks for courses taught at Wesleyan University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c707588-d5f6-4142-8289-da7ba6a97dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a8a68c-055e-4af0-ba4d-67d0d67148f1",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350712e-e145-4475-9588-e3f19469b5ce",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd2b4a-b890-465f-bf33-c2057738789e",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05de192-2baf-4d46-8848-6db1e3b50bff",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6519277-fc86-44fd-a75b-f74cad0efcf1",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}